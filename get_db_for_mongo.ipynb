{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Korean MongoDB\n",
    "\n",
    "### Создание базы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('ver_b.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['TRANSLIT', 'AUTO_GLOSS', 'ContKOR', 'RUS', 'COMMENT'], axis=1)[:477]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ADDRESS</th>\n",
       "      <th>KOR</th>\n",
       "      <th>BASE</th>\n",
       "      <th>SUF</th>\n",
       "      <th>GLOSS</th>\n",
       "      <th>TRANS</th>\n",
       "      <th>WClass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6a 1</td>\n",
       "      <td>夫부와</td>\n",
       "      <td>pwu</td>\n",
       "      <td>wa</td>\n",
       "      <td>COMIT</td>\n",
       "      <td>husband</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6a 1</td>\n",
       "      <td>婦부</td>\n",
       "      <td>pwu</td>\n",
       "      <td>non</td>\n",
       "      <td>TOP</td>\n",
       "      <td>wife</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6a 1</td>\n",
       "      <td>두</td>\n",
       "      <td>twu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6a 1</td>\n",
       "      <td>姓셩의</td>\n",
       "      <td>syeng</td>\n",
       "      <td>uy</td>\n",
       "      <td>GEN</td>\n",
       "      <td>family</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6a 1</td>\n",
       "      <td>合합홈이라</td>\n",
       "      <td>hap-ho</td>\n",
       "      <td>wo=m+i=la</td>\n",
       "      <td>VOL+NMNZ+COP+DECL</td>\n",
       "      <td>unite</td>\n",
       "      <td>N+COP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>473</td>\n",
       "      <td>13a 7</td>\n",
       "      <td>양짓믈고</td>\n",
       "      <td>yang-cic-mul-ho</td>\n",
       "      <td>kwo</td>\n",
       "      <td>COOR</td>\n",
       "      <td>brush teeth</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>474</td>\n",
       "      <td>13a 7</td>\n",
       "      <td>父부母모의</td>\n",
       "      <td>pwu-mwo</td>\n",
       "      <td>uy</td>\n",
       "      <td>GEN</td>\n",
       "      <td>parents</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>475</td>\n",
       "      <td>13a 7</td>\n",
       "      <td>고</td>\n",
       "      <td>kwot</td>\n",
       "      <td>oy</td>\n",
       "      <td>LOC</td>\n",
       "      <td>place</td>\n",
       "      <td>BN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>476</td>\n",
       "      <td>13a 7</td>\n",
       "      <td>가</td>\n",
       "      <td>ka</td>\n",
       "      <td>(a)</td>\n",
       "      <td>CV</td>\n",
       "      <td>go</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>477</td>\n",
       "      <td>13a 7</td>\n",
       "      <td>긔운을</td>\n",
       "      <td>kuy-wun</td>\n",
       "      <td>ul</td>\n",
       "      <td>ACC</td>\n",
       "      <td>strength, energy</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>477 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID ADDRESS    KOR             BASE        SUF              GLOSS  \\\n",
       "0      1    6a 1    夫부와              pwu         wa              COMIT   \n",
       "1      2    6a 1    婦부              pwu        non                TOP   \n",
       "2      3    6a 1      두              twu        NaN                NaN   \n",
       "3      4    6a 1    姓셩의            syeng         uy                GEN   \n",
       "4      5    6a 1  合합홈이라           hap-ho  wo=m+i=la  VOL+NMNZ+COP+DECL   \n",
       "..   ...     ...    ...              ...        ...                ...   \n",
       "472  473   13a 7  양짓믈고  yang-cic-mul-ho        kwo               COOR   \n",
       "473  474   13a 7  父부母모의          pwu-mwo         uy                GEN   \n",
       "474  475   13a 7     고             kwot         oy                LOC   \n",
       "475  476   13a 7      가               ka        (a)                 CV   \n",
       "476  477   13a 7    긔운을          kuy-wun         ul                ACC   \n",
       "\n",
       "                TRANS WClass  \n",
       "0             husband      N  \n",
       "1                wife      N  \n",
       "2                 two    NUM  \n",
       "3              family      N  \n",
       "4               unite  N+COP  \n",
       "..                ...    ...  \n",
       "472       brush teeth      P  \n",
       "473           parents      N  \n",
       "474             place     BN  \n",
       "475                go      P  \n",
       "476  strength, energy      N  \n",
       "\n",
       "[477 rows x 8 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['ADDRESS', 'KOR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marynepo/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/marynepo/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "page_lines = [re.findall('\\d+', pl) for pl in df['ADDRESS'].values]\n",
    "df['Page'] = [int(a[0]) for a in page_lines]\n",
    "df['Line'] = [int(a[1]) for a in page_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marynepo/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['Let_page'] = [re.findall('[ab]', adr)[0] for adr in df['ADDRESS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = 6\n",
    "pa = 'a'\n",
    "pl = 1\n",
    "npl = 1\n",
    "ls = [1]\n",
    "for p, a, l in zip(df['Page'][1:], df['Let_page'][1:], df['Line'][1:]):\n",
    "    if int(p) == pp and int(l) == pl and pa == a:\n",
    "        ls.append(npl)\n",
    "    elif int(p) == pp and int(l) != pl and pa == a:\n",
    "        ls.append(npl + 1)\n",
    "        npl = npl + 1\n",
    "    else:\n",
    "        ls.append(1)\n",
    "        npl = 1\n",
    "    pp = p\n",
    "    pl = l\n",
    "    pa = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marynepo/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['Line'] = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ADDRESS</th>\n",
       "      <th>KOR</th>\n",
       "      <th>BASE</th>\n",
       "      <th>SUF</th>\n",
       "      <th>GLOSS</th>\n",
       "      <th>TRANS</th>\n",
       "      <th>WClass</th>\n",
       "      <th>Page</th>\n",
       "      <th>Line</th>\n",
       "      <th>Let_page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>468</td>\n",
       "      <td>13a 6</td>\n",
       "      <td>이</td>\n",
       "      <td>tolk</td>\n",
       "      <td>i</td>\n",
       "      <td>NOM</td>\n",
       "      <td>chicken</td>\n",
       "      <td>N</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>469</td>\n",
       "      <td>13a 6</td>\n",
       "      <td>처음</td>\n",
       "      <td>che-um</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>beginning</td>\n",
       "      <td>N</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>470</td>\n",
       "      <td>13a 6</td>\n",
       "      <td>울거든</td>\n",
       "      <td>ul</td>\n",
       "      <td>ke-tun</td>\n",
       "      <td>CONC</td>\n",
       "      <td>cry</td>\n",
       "      <td>P</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>471</td>\n",
       "      <td>13a 6</td>\n",
       "      <td>다</td>\n",
       "      <td>ta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all</td>\n",
       "      <td>DET</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>472</td>\n",
       "      <td>13a 6</td>\n",
       "      <td>셰슈며</td>\n",
       "      <td>syey-sywu-ho</td>\n",
       "      <td>mye</td>\n",
       "      <td>COOR</td>\n",
       "      <td>wash one's face</td>\n",
       "      <td>P</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>473</td>\n",
       "      <td>13a 7</td>\n",
       "      <td>양짓믈고</td>\n",
       "      <td>yang-cic-mul-ho</td>\n",
       "      <td>kwo</td>\n",
       "      <td>COOR</td>\n",
       "      <td>brush teeth</td>\n",
       "      <td>P</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>474</td>\n",
       "      <td>13a 7</td>\n",
       "      <td>父부母모의</td>\n",
       "      <td>pwu-mwo</td>\n",
       "      <td>uy</td>\n",
       "      <td>GEN</td>\n",
       "      <td>parents</td>\n",
       "      <td>N</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>475</td>\n",
       "      <td>13a 7</td>\n",
       "      <td>고</td>\n",
       "      <td>kwot</td>\n",
       "      <td>oy</td>\n",
       "      <td>LOC</td>\n",
       "      <td>place</td>\n",
       "      <td>BN</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>476</td>\n",
       "      <td>13a 7</td>\n",
       "      <td>가</td>\n",
       "      <td>ka</td>\n",
       "      <td>(a)</td>\n",
       "      <td>CV</td>\n",
       "      <td>go</td>\n",
       "      <td>P</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>477</td>\n",
       "      <td>13a 7</td>\n",
       "      <td>긔운을</td>\n",
       "      <td>kuy-wun</td>\n",
       "      <td>ul</td>\n",
       "      <td>ACC</td>\n",
       "      <td>strength, energy</td>\n",
       "      <td>N</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID ADDRESS    KOR             BASE     SUF GLOSS             TRANS  \\\n",
       "467  468   13a 6     이             tolk       i   NOM           chicken   \n",
       "468  469   13a 6     처음           che-um     NaN   NaN         beginning   \n",
       "469  470   13a 6    울거든               ul  ke-tun  CONC               cry   \n",
       "470  471   13a 6      다               ta     NaN   NaN               all   \n",
       "471  472   13a 6   셰슈며     syey-sywu-ho     mye  COOR   wash one's face   \n",
       "472  473   13a 7  양짓믈고  yang-cic-mul-ho     kwo  COOR       brush teeth   \n",
       "473  474   13a 7  父부母모의          pwu-mwo      uy   GEN           parents   \n",
       "474  475   13a 7     고             kwot      oy   LOC             place   \n",
       "475  476   13a 7      가               ka     (a)    CV                go   \n",
       "476  477   13a 7    긔운을          kuy-wun      ul   ACC  strength, energy   \n",
       "\n",
       "    WClass  Page  Line Let_page  \n",
       "467      N    13     6        a  \n",
       "468      N    13     6        a  \n",
       "469      P    13     6        a  \n",
       "470    DET    13     6        a  \n",
       "471      P    13     6        a  \n",
       "472      P    13     7        a  \n",
       "473      N    13     7        a  \n",
       "474     BN    13     7        a  \n",
       "475      P    13     7        a  \n",
       "476      N    13     7        a  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = 6\n",
    "pl = 1\n",
    "pa = 'a'\n",
    "pages_dicts = []\n",
    "pag_lines = []\n",
    "line_cur = []\n",
    "w_pst = []\n",
    "for w,p,let,l in zip(df['KOR'], df['Page'], df['Let_page'], df['Line']):\n",
    "    if str(pp) + pa == str(p) + let:\n",
    "        if pl == int(l):\n",
    "            line_cur.append(w)\n",
    "        else:\n",
    "            pag_lines.append(' '.join(line_cur))\n",
    "            line_cur = [w]\n",
    "    else:\n",
    "        pag_lines.append(' '.join(line_cur))\n",
    "        pages_dicts.append({'page_num': pp, 'page_let': pa, 'lines': pag_lines})\n",
    "        pag_lines = []\n",
    "        line_cur = [w]\n",
    "    pp = int(p)\n",
    "    pl = int(l)\n",
    "    pa = let\n",
    "pages_dicts.append({'page_num': pp, 'page_let': pa, 'lines': pag_lines})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "suf_dicts = []\n",
    "words_dicts = []\n",
    "bases_dicts = []\n",
    "tokens_dicts = []\n",
    "k = 0\n",
    "pp = 6\n",
    "pa = a\n",
    "pl = 1\n",
    "for kr, w, tr, s, gl, cl, p, a, l in zip(df['KOR'], df['BASE'], df['TRANS'], df['SUF'], df['GLOSS'], df['WClass'], df['Page'], df['Let_page'], df['Line']):\n",
    "    if str(p) + a + str(l) != str(pp) + pa + str(pl):\n",
    "        k = 1\n",
    "    else:\n",
    "        k += 1\n",
    "    if str(s) in ['nan', '']:\n",
    "        sfs = [('-', '-')]\n",
    "        gls = ['-']\n",
    "    else:\n",
    "        if str(tr) in ['nan', '']:\n",
    "            tr = '?'\n",
    "        if str(gl) in ['nan', '']:\n",
    "            gl = '?'\n",
    "        if str(cl) in ['nan', '']:\n",
    "            cl = '?'        \n",
    "        ss = re.split('\\+|=', s)\n",
    "        gls = gl.split('+')\n",
    "        try:\n",
    "            assert len(ss) == len(gls)\n",
    "        except:\n",
    "            if gls == ['PROC', 'PARA']:\n",
    "                ss = ['no', 'ni']\n",
    "            elif gls == ['SHOULD', 'DECL']:\n",
    "                ss = ['l-ti-ni', 'la']\n",
    "            elif gls == ['PROC', 'PIND', 'DRV{N}', 'ACC']:\n",
    "                ss = ['no', 'n', 'i', 'lul']\n",
    "            elif gls == ['DECL', 'PIND']:\n",
    "                ss = ['la', 'n']\n",
    "            elif gls == ['HONSub', 'QUOT']:\n",
    "                ss = ['wo-sya', 'toy']\n",
    "            elif gls == ['SHOULD', 'DECL']:\n",
    "                ss = ['si-ni', 'la']\n",
    "            elif gls == ['?']:\n",
    "                ss = ['le-pse']\n",
    "            else:\n",
    "                print(ss, gls)\n",
    "        sfs = list(zip(ss, gls))\n",
    "    b_wc = cl\n",
    "    for g in gls[::-1]:\n",
    "        if gl == 'NMNZ' or gl == 'DRV{N}':\n",
    "            b_wc = 'N'\n",
    "            break\n",
    "        elif gl == 'ADV':\n",
    "            b_wc = 'ADV'\n",
    "            break\n",
    "        elif gl == 'COP':\n",
    "            b_wc = 'P'\n",
    "            break\n",
    "    bases_dicts.append((w, tr, cl))\n",
    "    suf_dicts.extend([(suf, g) for suf, g in sfs])\n",
    "    words_dicts.append((kr, b_wc, bases_dicts[-1], tuple([(suf,g) for suf, g in sfs])))\n",
    "    tokens_dicts.append({'word': words_dicts[-1], 'page': str(p) + a, 'line': int(l), 'line_pos': k})\n",
    "    pa = a\n",
    "    pl = l\n",
    "    pp = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "bases_dicts = set(bases_dicts)\n",
    "suf_dicts = set(suf_dicts)\n",
    "words_dicts = set(words_dicts)\n",
    "bds = []\n",
    "sds = []\n",
    "wds = []\n",
    "for w, tr, cl in bases_dicts:\n",
    "    bds.append({'base': w, 'trans': tr, 'pos': cl})\n",
    "for suf, g in suf_dicts:\n",
    "    sds.append({'suf': suf, 'gloss': g})\n",
    "for kr, wc, bd, sg in words_dicts:\n",
    "    wds.append({'kor': kr, 'pos': wc, 'lemma': bd, 'sufs': sg})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bases_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(suf_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('suf_dict.json', 'w') as f:\n",
    "    json.dump(sds, f)\n",
    "\n",
    "with open('base_dict.json', 'w') as f:\n",
    "    json.dump(bds, f)\n",
    "\n",
    "with open('word_dict.json', 'w') as f:\n",
    "    json.dump(wds, f)\n",
    "\n",
    "with open('token_dict.json', 'w') as f:\n",
    "    json.dump(tokens_dicts, f)\n",
    "    \n",
    "with open('pages_dict.json', 'w') as f:\n",
    "    json.dump(pages_dicts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/marynepo/anaconda3/envs/py37/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/marynepo/anaconda3/envs/py37/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: pymongo in ./anaconda3/envs/py37/lib/python3.7/site-packages (3.4.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/marynepo/anaconda3/envs/py37/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/marynepo/anaconda3/envs/py37/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/marynepo/anaconda3/envs/py37/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/marynepo/anaconda3/envs/py37/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/home/marynepo/anaconda3/envs/py37/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo==4.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подключение к базе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient('127.0.0.1', 27017)\n",
    "\n",
    "db = client['tonmon_sonsup_mongodb']\n",
    "pages = db['pages']\n",
    "words = db['words']\n",
    "bases = db['bases']\n",
    "tokens = db['tokens']\n",
    "sufs = db['sufixes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('suf_dict.json', 'r') as f:\n",
    "    sfs = json.load(f)\n",
    "\n",
    "with open('base_dict.json', 'r') as f:\n",
    "    bs = json.load(f)\n",
    "\n",
    "with open('word_dict.json', 'r') as f:\n",
    "    ws = json.load(f)\n",
    "\n",
    "with open('token_dict.json', 'r') as f:\n",
    "    tks = json.load(f)\n",
    "    \n",
    "with open('pages_dict.json', 'r') as f:\n",
    "    pgs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f8c5c51c870>"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages.insert_many(pgs)\n",
    "bases.insert_many(bs)\n",
    "sufs.insert_many(sfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in ws:\n",
    "    ss = []\n",
    "    for s in w['sufs']:\n",
    "        ss.append(sufs.find({'suf': s[0], 'gloss': s[1]})[0]['_id'])\n",
    "    w['sufs'] = ss\n",
    "    w['lemma'] = bases.find({'base': w['lemma'][0], 'trans': w['lemma'][1], 'pos': w['lemma'][2]})[0]['_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f8c5c545870>"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.insert_many(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in tks:\n",
    "    #print(w)\n",
    "    w['word'] = words.find(\n",
    "        {\n",
    "            'kor': w['word'][0],\n",
    "            'pos': w['word'][1],\n",
    "            'lemma': bases.find({'base': w['word'][2][0], 'trans': w['word'][2][1], 'pos': w['word'][2][2]})[0]['_id'],\n",
    "            'sufs': [sufs.find({'suf': s[0], 'gloss': s[1]})[0]['_id'] for s in w['word'][3]]\n",
    "        }\n",
    "    )[0]['_id']\n",
    "    w['page'] = pages.find({'page_num': int(w['page'][:-1]), 'page_let':w['page'][-1]})[0]['_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f8c5c4f2af0>"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.insert_many(tks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Демонстрация коллекций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('624431d50051766d9df58933'),\n",
      " 'lines': ['夫부와 婦부\\ue285 두 姓셩의 合합홈이라',\n",
      "           '生\\ue9a2民민의 비로 \\ue98b이며 萬만福복의',\n",
      "           '근원이니 듕\\ue57b를 行\\uf55b\\uf537야 혼인을 의논\\uf537며',\n",
      "           '폐\\ue669을 드리고 친히 마즘은 그',\n",
      "           '別별홈을 두터이홈이라 이런 故고로',\n",
      "           '妻쳐를 娶\\uf325호\\ue3a8 同동姓셩을 娶\\uf325티',\n",
      "           '아니\\uf537며 宮궁室실을 지으\\ue3a8 안히며'],\n",
      " 'page_let': 'a',\n",
      " 'page_num': 6}\n",
      "{'_id': ObjectId('624431d50051766d9df58934'),\n",
      " 'lines': ['밧\\uea35 분변\\uf537야 男남子\\uf1fc\\ue285 밧긔 이셔',\n",
      "           '안흘 니르디 아니\\uf537고 婦부人인은 안\\uf550',\n",
      "           '이셔 밧\\uea35 니르디 아니\\uf537\\ue283니 진실로',\n",
      "           '能능히 莊장으로\\ue73b 涖니\\uf537야\\ue73b 하\\ue288의',\n",
      "           '健건\\uf53a 道도를 體톄\\uf537고 부드러옴으로\\ue73b',\n",
      "           '正졍히 \\uf537야 \\ue73b \\uea71희 順슌\\uf53a',\n",
      "           '義의를 니으면 집 道도ㅣ 正졍\\uf537려니와'],\n",
      " 'page_let': 'b',\n",
      " 'page_num': 6}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for i in pages.find()[:2]:\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('624431d90051766d9df58c5b'),\n",
      " 'line': 1,\n",
      " 'line_pos': 1,\n",
      " 'page': ObjectId('624431d50051766d9df58933'),\n",
      " 'word': ObjectId('624431d70051766d9df58c1e')}\n",
      "{'_id': ObjectId('624431d90051766d9df58c5c'),\n",
      " 'line': 1,\n",
      " 'line_pos': 2,\n",
      " 'page': ObjectId('624431d50051766d9df58933'),\n",
      " 'word': ObjectId('624431d70051766d9df58b66')}\n"
     ]
    }
   ],
   "source": [
    "for i in tokens.find()[:2]:\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('624431d70051766d9df58ad1'),\n",
      " 'kor': '니으면',\n",
      " 'lemma': ObjectId('624431d50051766d9df58964'),\n",
      " 'pos': 'P',\n",
      " 'sufs': [ObjectId('624431d50051766d9df58a81')]}\n",
      "{'_id': ObjectId('624431d70051766d9df58ad2'),\n",
      " 'kor': '우희',\n",
      " 'lemma': ObjectId('624431d50051766d9df58a66'),\n",
      " 'pos': 'N',\n",
      " 'sufs': [ObjectId('624431d50051766d9df58ab7')]}\n"
     ]
    }
   ],
   "source": [
    "for i in words.find()[:2]:\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('624431d50051766d9df58942'),\n",
      " 'base': 'hoy-tey',\n",
      " 'pos': 'N',\n",
      " 'trans': 'baby'}\n",
      "{'_id': ObjectId('624431d50051766d9df58943'),\n",
      " 'base': 'cyel-cyel-ho',\n",
      " 'pos': 'P',\n",
      " 'trans': 'eager, ardent'}\n"
     ]
    }
   ],
   "source": [
    "for i in bases.find()[:2]:\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('624431d50051766d9df58a77'), 'gloss': 'PROB', 'suf': 'li'}\n",
      "{'_id': ObjectId('624431d50051766d9df58a78'), 'gloss': '?', 'suf': 'thyey-lwo'}\n"
     ]
    }
   ],
   "source": [
    "for i in sufs.find()[:2]:\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('624431d50051766d9df58a78'), 'gloss': '?', 'suf': 'thyey-lwo'}\n",
      "{'_id': ObjectId('624431d50051766d9df58a85'), 'gloss': '?', 'suf': 'l-ti-ni-la'}\n",
      "{'_id': ObjectId('624431d50051766d9df58a8a'), 'gloss': '?', 'suf': 'nun'}\n",
      "{'_id': ObjectId('624431d50051766d9df58a9a'), 'gloss': '?', 'suf': 'e'}\n",
      "{'_id': ObjectId('624431d50051766d9df58aa0'), 'gloss': '?', 'suf': 'twoy'}\n",
      "{'_id': ObjectId('624431d50051766d9df58aaf'), 'gloss': '?', 'suf': 'le-pse'}\n"
     ]
    }
   ],
   "source": [
    "for i in sufs.find({'gloss': '?'}):\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x7f8c5f93fa00>"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sufs.update_many(\n",
    "  {'gloss': '?'},\n",
    "  {'$set': {'gloss': 'UNK'}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gloss': 'UNK', 'suf': 'thyey-lwo'}\n",
      "{'gloss': 'UNK', 'suf': 'l-ti-ni-la'}\n",
      "{'gloss': 'UNK', 'suf': 'nun'}\n",
      "{'gloss': 'UNK', 'suf': 'e'}\n",
      "{'gloss': 'UNK', 'suf': 'twoy'}\n",
      "{'gloss': 'UNK', 'suf': 'le-pse'}\n"
     ]
    }
   ],
   "source": [
    "for i in sufs.find({'gloss': 'UNK'}, projection={'_id': False}):\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate и все остальное\n",
    "\n",
    "#### Частотность суффиксов. Создание нового ключа для ее хранения в коллекции суффиксов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in words.aggregate([\n",
    "    \n",
    "    {\n",
    "        '$unwind': '$sufs'\n",
    "    },\n",
    "    {\n",
    "        '$lookup': {\n",
    "            'from': 'sufixes',\n",
    "            'localField': 'sufs',\n",
    "            'foreignField':'_id',\n",
    "            'as': 'sufxs'\n",
    "        }},\n",
    "    {'$unwind': '$sufxs'},\n",
    "    {\n",
    "        '$group': {\n",
    "            '_id': '$sufs',\n",
    "            'suf': {'$first':'$sufxs.suf'},\n",
    "            'gloss': {'$first':'$sufxs.gloss'},\n",
    "            'count': { '$sum': 1 }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$sort': {'count': -1}\n",
    "    },\n",
    "    {'$project': {'_id': 0, 'suf': 1, 'gloss': 1, 'count': 1}},\n",
    "\n",
    "    ]):\n",
    "    sufs.find_one_and_update({'suf': i['suf'], 'gloss': i['gloss']},\n",
    "                             {'$set': {'freq': i['count']}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suffix: li, gloss: PROB, freq: 7\n",
      "Suffix: thyey-lwo, gloss: UNK, freq: 2\n",
      "Suffix: oy, gloss: LOC, freq: 2\n",
      "Suffix: un, gloss: TOP, freq: 6\n",
      "Suffix: l-ti-ni, gloss: SHOULD, freq: 5\n"
     ]
    }
   ],
   "source": [
    "for i in sufs.find()[:5]:\n",
    "    print('Suffix: ' + i['suf'] + ', gloss: ' + i['gloss'] + ', freq: ' + str(i['freq']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert и Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('62442eed0051766d9df5842d'),\n",
      " 'freq': 0,\n",
      " 'gloss': 'lsoy',\n",
      " 'suf': 'CAUS'}\n",
      "{'_id': ObjectId('62442eed0051766d9df5842e'),\n",
      " 'freq': 0,\n",
      " 'gloss': 'ta',\n",
      " 'suf': 'DECL'}\n"
     ]
    }
   ],
   "source": [
    "zero_suf = [\n",
    "    {'suf': 'CAUS', 'gloss': 'lsoy', 'freq': 0},\n",
    "    {'suf': 'DECL', 'gloss': 'ta', 'freq': 0}\n",
    "]\n",
    "\n",
    "sufs.insert_many(zero_suf)\n",
    "for i in sufs.find({'freq': 0}):\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "sufs.delete_many({'freq': {'$eq': 0}}) \n",
    "for i in sufs.find({'freq': 0}):\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Группировка суффиксов по глоссам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gloss: CONC, suffixes: ni-wa, u-toy, ke-tun, wotoy, toy, n-ti-la, wo-toy, n-tuy, non-ti-la, number of suffixes: 9\n",
      "Gloss: CV, suffixes: e, ya, ya-pse, ya-sa, (a), a, number of suffixes: 6\n",
      "Gloss: LOC, suffixes: oy, ul, ey, yey, ay, uy, number of suffixes: 6\n",
      "Gloss: UNK, suffixes: thyey-lwo, l-ti-ni-la, nun, e, twoy, le-pse, number of suffixes: 6\n",
      "Gloss: HONSub, suffixes: si-wo, wo-sya, si, wo-si, number of suffixes: 4\n",
      "Gloss: INST, suffixes: u-lwo-pse, lwo, u-lwo, lwo-pse, number of suffixes: 4\n",
      "Gloss: COND, suffixes: u-myen, khe-tun, myen, ke-tun, number of suffixes: 4\n",
      "Gloss: PIND, suffixes: un, n, nun, non, number of suffixes: 4\n",
      "Gloss: NEG, suffixes: thi, u-ti, ti, number of suffixes: 3\n",
      "Gloss: SHOULD, suffixes: l-ti-ni, l-ti-la, ul-ti-ni, number of suffixes: 3\n",
      "Gloss: COOR, suffixes: mye, kwo, u-mye, number of suffixes: 3\n",
      "Gloss: QUOT, suffixes: toy, wo-toy, number of suffixes: 2\n",
      "Gloss: ACC, suffixes: lul, ul, number of suffixes: 2\n",
      "Gloss: DAT, suffixes: ey, uy-key, number of suffixes: 2\n",
      "Gloss: CAUS, suffixes: ke-nul, hi, number of suffixes: 2\n",
      "Gloss: COMIT, suffixes: wa, kwa, number of suffixes: 2\n",
      "Gloss: PARA, suffixes: ni, u-ni, number of suffixes: 2\n",
      "Gloss: INTER, suffixes: n-ta, wo, number of suffixes: 2\n",
      "Gloss: DUB, suffixes: lye, li, number of suffixes: 2\n",
      "Gloss: NMNZ, suffixes: m, ki, number of suffixes: 2\n",
      "Gloss: TOP, suffixes: un, non, number of suffixes: 2\n"
     ]
    }
   ],
   "source": [
    "for i in sufs.aggregate([\n",
    "    \n",
    "    {\n",
    "        '$group': {\n",
    "            '_id': '$gloss',\n",
    "            'sufs': {'$push': '$suf'},\n",
    "            'count': { '$sum': 1 }\n",
    "        }\n",
    "    },\n",
    "    {'$match': {'count': {'$gt': 1}}},\n",
    "    {'$sort': {'count': -1}},\n",
    "    ]):\n",
    "    print('Gloss: ' + i['_id'] + ', suffixes: ' + ', '.join(i['sufs']) + ', number of suffixes: ' + str(i['count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Глоссировка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      "6a 1\n",
      "***\n",
      "夫부와 婦부 두 姓셩의 合합홈이라\n",
      "husband-COMIT wife-TOP two family-GEN unite-VOL-NMNZ-COP-DECL\n",
      "***\n",
      "6a 2\n",
      "***\n",
      "生民민의 비로 이며 萬만福복의\n",
      "people-GEN birth-INST beginning-COOR great fortune-GEN\n",
      "***\n",
      "6a 3\n",
      "***\n",
      "근원이니 듕를 行야 혼인을 의논며\n",
      "source-PARA matchmaking-ACC do-CV marriage-ACC discuss-COOR\n"
     ]
    }
   ],
   "source": [
    "for i in tokens.aggregate([\n",
    "    {\n",
    "        '$lookup': {\n",
    "            'from': 'words',\n",
    "            'localField': 'word',\n",
    "            'foreignField':'_id',\n",
    "            'as': 'word'\n",
    "        }},\n",
    "    {\"$unwind\": \"$word\"},\n",
    "    {\n",
    "        '$lookup': {\n",
    "            'from': 'bases',\n",
    "            'localField': 'word.lemma',\n",
    "            'foreignField':'_id',\n",
    "            'as': 'base'\n",
    "        }},\n",
    "    {\"$unwind\": \"$base\"},\n",
    "    {'$unwind': '$word.sufs'},\n",
    "    {\n",
    "        '$lookup': {\n",
    "            'from': 'sufixes',\n",
    "            'localField': 'word.sufs',\n",
    "            'foreignField':'_id',\n",
    "            'as': 'sufxs'\n",
    "        }},\n",
    "    {\"$unwind\": \"$sufxs\"},\n",
    "    { \"$group\": {\n",
    "        \"_id\": {\n",
    "            \"page\": \"$page\",\n",
    "            \"line\": \"$line\",\n",
    "            \"line_pos\": \"$line_pos\"},\n",
    "        \"gloss\": {\"$push\": \"$sufxs.gloss\"},\n",
    "        \"trans\": {\"$first\": \"$base.trans\"},\n",
    "        }},\n",
    "    {\n",
    "        '$lookup': {\n",
    "            'from': 'pages',\n",
    "            'localField': '_id.page',\n",
    "            'foreignField':'_id',\n",
    "            'as': 'page'\n",
    "        }},\n",
    "    {\"$unwind\": '$page'},\n",
    "    {\"$sort\": {'page.page_num': 1, 'page.page_let': 1, \"_id.line\": 1, \"_id.line_pos\": 1}},\n",
    "    { \"$group\": {\n",
    "        \"_id\": {\n",
    "            \"page_num\": \"$page.page_num\",\n",
    "            \"page_let\": \"$page.page_let\",\n",
    "            \"line\": \"$_id.line\"},\n",
    "        \"gloss\": {\"$push\": \"$gloss\"},\n",
    "        \"trans\": {\"$push\": \"$trans\"},\n",
    "        \"lines\": {\"$first\": '$page.lines'}}},\n",
    "    {\"$sort\": {'_id.page_num': 1, '_id.page_let': 1, \"_id.line\": 1}},\n",
    "    {\"$limit\": 3}\n",
    "        ,]):\n",
    "    print('***')\n",
    "    print(str(i['_id']['page_num']) + i['_id']['page_let'] + ' ' + str(i['_id']['line']))\n",
    "    print('***')\n",
    "    print(i['lines'][i['_id']['line'] - 1])\n",
    "    line = []\n",
    "    for trans, gl in zip(i['trans'], i['gloss']):\n",
    "        if gl != ['-']:\n",
    "            line.append(trans.split(',')[0] + '-'+ '-'.join(gl))\n",
    "        else:\n",
    "            line.append(trans.split(',')[0])\n",
    "    print(' '.join(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Работа с текстами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'suf_text'"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import TEXT\n",
    "\n",
    "bases.create_index([(\"base\", TEXT), (\"trans\", TEXT)])\n",
    "sufs.create_index([(\"suf\", TEXT)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base': 'pwu-pwu', 'pos': 'N', 'trans': 'husband and wife'}\n"
     ]
    }
   ],
   "source": [
    "for i in bases.aggregate(\n",
    "   [\n",
    "     {\"$match\": {\"$text\": {\"$search\": \"\\\"husband and\\\"\"}}},\n",
    "     {\"$sort\": {\"score\":{\"$meta\": \"textScore\"}}},\n",
    "     {\"$project\": {\"_id\": 0, \"base\": 1, \"trans\": 1, \"pos\": 1}}\n",
    "   ]\n",
    "):\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base': 'pwu', 'pos': 'N', 'trans': 'wife'}\n",
      "{'base': 'chye', 'pos': 'N', 'trans': 'wife'}\n",
      "{'base': 'an-hay', 'pos': 'N', 'trans': 'wife'}\n",
      "{'base': 'pwu', 'pos': 'N', 'trans': \"wife'\"}\n"
     ]
    }
   ],
   "source": [
    "for i in bases.aggregate(\n",
    "   [\n",
    "     {\"$match\": {\"$text\": {\"$search\": \"wife -husband\"}}},\n",
    "     {\"$sort\": {\"score\":{\"$meta\": \"textScore\"}}},\n",
    "     {\"$project\": {\"_id\": 0, \"base\": 1, \"trans\": 1, \"pos\": 1}}\n",
    "   ]\n",
    "):\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base': 'hywo-co', 'pos': 'N', 'trans': 'dutiful son'}\n",
      "{'base': 'thyen-co', 'pos': 'N', 'trans': 'the son of God'}\n",
      "{'base': 'two-li', 'pos': 'N', 'trans': 'duty, way'}\n"
     ]
    }
   ],
   "source": [
    "for i in bases.aggregate(\n",
    "   [\n",
    "     {\"$match\": {\"$text\": {\"$search\": \"duty son\"}}},\n",
    "     {\"$sort\": {\"score\":{\"$meta\": \"textScore\"}}},\n",
    "     {\"$project\": {\"_id\": 0, \"base\": 1, \"trans\": 1, \"pos\": 1}}\n",
    "   ]\n",
    "):\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regex\n",
    "\n",
    "##### Строчки, содержащие 벋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "세 가짇 벋이오 해로오니 세 가짇\n",
      "니를 벋며 便편佞령니를 벋면\n",
      "샤 벋의게 믿브디 몯면\n"
     ]
    }
   ],
   "source": [
    "for i in pages.find({'lines': {'$elemMatch': {'$regex': '.+벋.+'}}}, {'_id': 0, 'lines.$': 1}):\n",
    "    print('\\n'.join(i['lines']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Перевод и корейское написание одной из форм основ на -ho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base': 'cyel-cyel-ho',\n",
      " 'trans': 'eager, ardent',\n",
      " 'word': {'kor': '切졀切졀\\uf537며'}}\n",
      "{'base': 'thyey-ho',\n",
      " 'trans': 'personally do, take shape',\n",
      " 'word': {'kor': '體톄\\uf537고'}}\n",
      "{'base': 'kwong-kyeng-ho', 'trans': 'respectful', 'word': {'kor': '공경\\uf537니'}}\n",
      "{'base': 'toy-cyep-ho', 'trans': 'treat', 'word': {'kor': '\\ue3a8졉홈을'}}\n",
      "{'base': 'syey-sywu-ho',\n",
      " 'trans': \"wash one's face\",\n",
      " 'word': {'kor': '셰슈\\uf537며'}}\n"
     ]
    }
   ],
   "source": [
    "for i in bases.aggregate([\n",
    "        {'$match': {'base': {'$regex': '.+\\-ho'}}},\n",
    "        {'$lookup': {\n",
    "            'from': 'words',\n",
    "            'localField': '_id',\n",
    "            'foreignField':'lemma',\n",
    "            'as': 'word'\n",
    "        }},\n",
    "        {'$unwind': '$word'},\n",
    "        {'$project': {'_id': 0, 'word.kor': 1, 'trans': 1, 'base': 1}},\n",
    "        {'$limit': 5}\n",
    "    ]\n",
    "\n",
    "):\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
